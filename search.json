[{"path":"https://docs.ropensci.org/arkdb/CODE_OF_CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http://contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":"https://docs.ropensci.org/arkdb/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2018 Carl Boettiger Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"package-rationale","dir":"Articles","previous_headings":"arkdb","what":"Package rationale","title":"Introduction to arkdb","text":"Increasing data sizes create challenges fundamental tasks publishing, distributing, preserving data. Despite (perhaps ) diverse ever-expanding number database file formats, humble plain text file comma tab-separated-values (e.g. .csv .tsv files) remains gold standard data archiving distribution. files can read almost platform tool can efficiently compressed using long-standing widely available standard open source libraries like gzip bzip2. contrast, database storage formats dumps usually particular database platform used generate , likely compatible different database engines (e.g. PostgreSQL -> SQLite) even different versions engine. Researchers unfamiliar databases difficulty accessing data, dumps may also formats less efficient compress. Working tables large working memory machines using external relational database stores now common R practice, thanks ever-rising availability data increasing support popularity packages DBI, dplyr, dbplyr. Working plain text files becomes increasingly difficult context. Many R users sufficient RAM simply read 10 GB .tsv file R. Similarly, moving 10 GB database relational data file plain text file archiving distribution similarly challenging R. relational database back-ends implement form COPY IMPORT allows read export plain text files directly, methods consistent across database types part standard SQL interface. importantly case, also called directly R, require separate stand-alone installation database client. arkdb provides simple solution two tasks. goal arkdb provide convenient way move data large compressed text files (e.g. *.tsv.bz2) DBI-compliant database connection (see DBI), move tables databases text files. key feature arkdb files moved databases text files chunks fixed size, allowing package functions work tables much large read memory . slower reading file memory one go, can scaled larger data larger data additional memory requirement.","code":""},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"installation","dir":"Articles","previous_headings":"arkdb","what":"Installation","title":"Introduction to arkdb","text":"can install arkdb GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"cboettig/arkdb\")"},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"tutorial","dir":"Articles","previous_headings":"","what":"Tutorial","title":"Introduction to arkdb","text":"","code":"library(arkdb)  # additional libraries just for this demo library(dbplyr) library(dplyr) library(nycflights13) library(fs)"},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"creating-an-archive-of-an-existing-database","dir":"Articles","previous_headings":"Tutorial","what":"Creating an archive of an existing database","title":"Introduction to arkdb","text":"First, ’ll need example database work . Conveniently, nice example using NYC flights data built dbplyr package. create archive, just give ark connection database tell want *.tsv.bz2 files archived. can also set chunk size number lines read single chunk. lines per chunk usually means faster run time cost higher memory requirements. can take look confirm files written. Note can use fs::dir_info get nice snapshot file sizes. Compare compressed sizes original database:","code":"tmp <- tempdir() # Or can be your working directory, \".\" db <- dbplyr::nycflights13_sqlite(tmp) #> Caching nycflights db at /tmp/Rtmp71jA3Z/nycflights13.sqlite #> Creating table: airlines #> Creating table: airports #> Creating table: flights #> Creating table: planes #> Creating table: weather dir <- fs::dir_create(fs::path(tmp, \"nycflights\")) ark(db, dir, lines = 50000) #> Exporting airlines in 50000 line chunks: #>  ...Done! (in 0.003984928 secs) #> Exporting airports in 50000 line chunks: #>  ...Done! (in 0.01179194 secs) #> Exporting flights in 50000 line chunks: #>  ...Done! (in 5.810223 secs) #> Exporting planes in 50000 line chunks: #>  ...Done! (in 0.01563001 secs) #> Exporting weather in 50000 line chunks: #>  ...Done! (in 0.4196997 secs) fs::dir_info(dir) %>%    select(path, size) %>%   mutate(path = fs::path_file(path)) #> # A tibble: 5 × 2 #>   path                    size #>   <chr>            <fs::bytes> #> 1 airlines.tsv.bz2         260 #> 2 airports.tsv.bz2      28.13K #> 3 flights.tsv.bz2        4.85M #> 4 planes.tsv.bz2        11.96K #> 5 weather.tsv.bz2      278.84K  fs::file_info(fs::path(tmp,\"nycflights13.sqlite\")) %>%    pull(size) #> 44.9M"},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"unarchive","dir":"Articles","previous_headings":"Tutorial","what":"Unarchive","title":"Introduction to arkdb","text":"Now ’ve gotten database (compressed) plain text files, let’s get back . simply need pass unark list compressed files connection database. create new local SQLite database. Note design means also easy use arkdb move data databases. ark, can set chunk size control memory footprint required: unark returns dplyr database connection can use usual way:","code":"files <- fs::dir_ls(dir, glob = \"*.tsv.bz2\") new_db <- DBI::dbConnect(RSQLite::SQLite(), fs::path(tmp, \"local.sqlite\")) unark(files, new_db, lines = 50000)   #> Importing /tmp/Rtmp71jA3Z/nycflights/airlines.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.007319927 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/airports.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.01323366 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/flights.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 3.509614 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/planes.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.01924562 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/weather.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.1440406 secs) tbl(new_db, \"flights\") #> # Source:   table<flights> [?? x 19] #> # Database: sqlite 3.45.0 [/tmp/Rtmp71jA3Z/local.sqlite] #>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time #>    <int> <int> <int>    <int>          <int>     <int>    <int>          <int> #>  1  2013     1     1      517            515         2      830            819 #>  2  2013     1     1      533            529         4      850            830 #>  3  2013     1     1      542            540         2      923            850 #>  4  2013     1     1      544            545        -1     1004           1022 #>  5  2013     1     1      554            600        -6      812            837 #>  6  2013     1     1      554            558        -4      740            728 #>  7  2013     1     1      555            600        -5      913            854 #>  8  2013     1     1      557            600        -3      709            723 #>  9  2013     1     1      557            600        -3      838            846 #> 10  2013     1     1      558            600        -2      753            745 #> # ℹ more rows #> # ℹ 11 more variables: arr_delay <int>, carrier <chr>, flight <int>, #> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <int>, distance <int>, #> #   hour <int>, minute <int>, time_hour <dbl> # Remove example files we created. DBI::dbDisconnect(new_db) unlink(dir, TRUE) unlink(fs::path(tmp, \"local.sqlite\"))"},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"pluggable-text-formats","dir":"Articles","previous_headings":"Tutorial","what":"Pluggable text formats","title":"Introduction to arkdb","text":"default, arkdb uses tsv format, implemented base tools, text-based serialization. tsv standard particularly attractive side-steps ambiguities present CSV format due string quoting. IANA Standard TSV neatly avoids tab-separated values insisting tab can ever separator. arkdb provides pluggable mechanism changing back end utility used write text files. instance, need read export .csv format, can simply swap csv based reader ark() unark() methods, illustrated : arkdb also provides function streamable_table() facilitate users creating streaming table interfaces. instance, prefer use readr methods read write tsv files, construct table follows (streamable_readr_tsv() streamable_readr_csv() also shipped inside arkdb convenience): can pass streamable table directly ark() unark(), like : Note several constraints design. write method must able take generic R connection object (allow handle compression methods used, ), read method must able take textConnection object. readr functions handle cases box, method easy write. Also note write method must able append, .e. use header append=TRUE, omit FALSE. See built-methods examples.","code":"dir <- fs::dir_create(fs::path(tmp, \"nycflights\"))  ark(db, dir,      streamable_table = streamable_base_csv()) #> Exporting airlines in 50000 line chunks: #>  ...Done! (in 0.001810789 secs) #> Exporting airports in 50000 line chunks: #>  ...Done! (in 0.01095533 secs) #> Exporting flights in 50000 line chunks: #>  ...Done! (in 6.050243 secs) #> Exporting planes in 50000 line chunks: #>  ...Done! (in 0.0166409 secs) #> Exporting weather in 50000 line chunks: #>  ...Done! (in 0.4223397 secs) files <- fs::dir_ls(dir, glob = \"*.csv.bz2\") new_db <- DBI::dbConnect(RSQLite::SQLite(), fs::path(tmp, \"local.sqlite\"))  unark(files, new_db,       streamable_table = streamable_base_csv()) #> Importing /tmp/Rtmp71jA3Z/nycflights/airlines.csv.bz2 in 50000 line chunks: #>  ...Done! (in 0.007172108 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/airports.csv.bz2 in 50000 line chunks: #>  ...Done! (in 0.01322842 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/flights.csv.bz2 in 50000 line chunks: #>  ...Done! (in 3.673843 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/planes.csv.bz2 in 50000 line chunks: #>  ...Done! (in 0.01932621 secs) #> Importing /tmp/Rtmp71jA3Z/nycflights/weather.csv.bz2 in 50000 line chunks: #>  ...Done! (in 0.1897376 secs) stream <-     streamable_table(      function(file, ...) readr::read_tsv(file, ...),      function(x, path, omit_header)        readr::write_tsv(x = x, path = path, append = omit_header),      \"tsv\") ark(db, dir,      streamable_table = stream) #> Exporting airlines in 50000 line chunks: #> Warning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0. #> ℹ Please use the `file` argument instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. #>  ...Done! (in 0.1260805 secs) #> Exporting airports in 50000 line chunks: #>  ...Done! (in 0.05660677 secs) #> Exporting flights in 50000 line chunks: #>  ...Done! (in 2.95391 secs) #> Exporting planes in 50000 line chunks: #>  ...Done! (in 0.03582048 secs) #> Exporting weather in 50000 line chunks: #>  ...Done! (in 0.2343223 secs)"},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"a-note-on-compression","dir":"Articles","previous_headings":"Tutorial","what":"A note on compression","title":"Introduction to arkdb","text":"unark can read variety compression formats recognized base R: bzip2, gzip, zip, xz, ark can choose compression algorithm. Note trade-speed compression efficiency (.e. final file size). ark uses bz2 compression algorithm default, supported base R, compress tsv files. bz2 offers excellent compression levels, considerably slower compress gzip zip. comparably fast uncompress. faster archiving maximum file size reduction critical, gzip give nearly effective compression significantly less time. Compression can also turned , e.g. using ark() compress=\"none\" unark() files compression suffix (e.g. *.tsv instead *.tsv.gz).","code":""},{"path":"https://docs.ropensci.org/arkdb/articles/arkdb.html","id":"distributing-data","dir":"Articles","previous_headings":"Tutorial","what":"Distributing data","title":"Introduction to arkdb","text":"archived database files ark, consider sharing privately publicly part project GitHub repo using piggyback R package. permanent, versioned, citable data archiving, upload *.tsv.bz2 files data repository like Zenodo.org.","code":""},{"path":"https://docs.ropensci.org/arkdb/articles/noapi.html","id":"draft-post","dir":"Articles","previous_headings":"","what":"DRAFT POST","title":"Working with medium-sized data","text":"past summer, written two small-ish R packages address challenges frequently run course research. challenges refer medium-sized data – kind petabyte scale “big data” precludes analysis standard hardware existing methodology, large enough size alone starts creating problems certain bits typical workflow. precisely, take medium-sized refer data large comfortably fit memory laptops (e.g. order several GB), data merely large commit GitHub. typical workflow, mean easily able share parts analysis publicly privately collaborators (merely different machines, laptop cloud server) able reproduce results minimal fuss configuration. data large fit memory, ’s already well-established solution using external database, store data. Thanks dplyr’s database backends, many R users can adapt workflow relatively seamlessly move dplyr commands call -memory data frames identical nearly identical commands call database. works pretty well data already database, getting onto database, moving data around people/machines can access nearly straight forward. far, part problem received relatively little attention. reason usual response problem “’re wrong.” standard practice context simply move data . central database server, usually access controlled password credential, can allow multiple users query database directly. Thanks magical abstractions SQL queries DBI package, user (aka client), doesn’t need care details database located, even particular backend used. Moving data around can slow expensive. Arbitrarily large data can housed central/cloud location provisioned enough resources store everything process complex queries. Consequently, just every database backend provides mechanism SQL / dplyr querying, filtering, joining etc data fit memory , also nearly every backend provides server abilities network connection, handling secure logins forth. want anything else? problem usual response often odds original objectives typical scientific workflows. Setting database server can non-trivial; mean: difficult automate portable/cross-platform manner working entirely R. importantly, reflects use-case typical industry context scientific practice. Individual researchers need make data available global community scientists can reproduce results years decades later; just handful employees can granted authenticated access central database. Archiving data static text files far scalable, cost-effective (storing static files much cheaper keeping database server running), future-proof (rapid evolution database technology always backwards compatible) simplifies avoids security issues involved maintaining public server. scientific context, almost always makes sense move data . Scientific data repositories already built precisely model: providing long term storage files can downloaded analyzed locally. smaller .csv files, works pretty well. just wanted access data bit larger active memory. fact widely-used solution case: Lite flavors databases like SQLite, new favorite, MonetDBLite, provide disk-based storage support network connection server model. Using corresponding R packages, databases can easily deployed store & query data local disk.","code":""},{"path":"https://docs.ropensci.org/arkdb/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Carl Boettiger. Author, maintainer, copyright holder. Richard FitzJohn. Contributor. Brandon Bertelsen. Contributor.","code":""},{"path":"https://docs.ropensci.org/arkdb/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Boettiger C (2024). arkdb: Archive Unarchive Databases Using Flat Files. R package version 0.0.18, https://docs.ropensci.org/arkdb/, https://github.com/ropensci/arkdb.","code":"@Manual{,   title = {arkdb: Archive and Unarchive Databases Using Flat Files},   author = {Carl Boettiger},   year = {2024},   note = {R package version 0.0.18, https://docs.ropensci.org/arkdb/},   url = {https://github.com/ropensci/arkdb}, }"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"arkdb-","dir":"","previous_headings":"","what":"Archive and Unarchive Databases Using Flat Files","title":"Archive and Unarchive Databases Using Flat Files","text":"goal arkdb provide convenient way move data large compressed text files (tsv, csv, etc) DBI-compliant database connection (e.g. MYSQL, Postgres, SQLite; see DBI), move tables databases text files. key feature arkdb files moved databases text files chunks fixed size, allowing package functions work tables much large read memory . also functionality filtering applying transformation data extracted database. arkdb package easily extended use custom read write methods allowing dictate output formats. See R/streamable_table.R examples include using: Base c/tsv Apache arrow’s parquet readr package c/tsv","code":""},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"links","dir":"","previous_headings":"","what":"Links","title":"Archive and Unarchive Databases Using Flat Files","text":"detailed introduction package design use can found package Vignette Online versions package documentation","code":""},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Archive and Unarchive Databases Using Flat Files","text":"can install arkdb GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"cboettig/arkdb\")"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"basic-use","dir":"","previous_headings":"","what":"Basic use","title":"Archive and Unarchive Databases Using Flat Files","text":"","code":"library(arkdb)  # additional libraries just for this demo library(dbplyr) library(dplyr) library(fs)"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"creating-an-archive-of-a-database","dir":"","previous_headings":"","what":"Creating an archive of a database","title":"Archive and Unarchive Databases Using Flat Files","text":"Consider nycflights database SQLite: Create archive database:","code":"tmp <- tempdir() # Or can be your working directory, \".\" db <- dbplyr::nycflights13_sqlite(tmp) #> Caching nycflights db at /tmp/Rtmpm6YZ0e/nycflights13.sqlite #> Creating table: airlines #> Creating table: airports #> Creating table: flights #> Creating table: planes #> Creating table: weather dir <- fs::dir_create(fs::path(tmp, \"nycflights\")) ark(db, dir, lines = 50000) #> Exporting airlines in 50000 line chunks: #>  ...Done! (in 0.005531788 secs) #> Exporting airports in 50000 line chunks: #>  ...Done! (in 0.02239442 secs) #> Exporting flights in 50000 line chunks: #>  ...Done! (in 11.78997 secs) #> Exporting planes in 50000 line chunks: #>  ...Done! (in 0.03349638 secs) #> Exporting weather in 50000 line chunks: #>  ...Done! (in 0.8155148 secs)"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"unarchive","dir":"","previous_headings":"","what":"Unarchive","title":"Archive and Unarchive Databases Using Flat Files","text":"Import list compressed tabular files (.e. *.csv.bz2) local SQLite database:","code":"files <- fs::dir_ls(dir) new_db <- DBI::dbConnect(RSQLite::SQLite(), fs::path(tmp, \"local.sqlite\"))  unark(files, new_db, lines = 50000) #> Importing /tmp/Rtmpm6YZ0e/nycflights/airlines.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.0131464 secs) #> Importing /tmp/Rtmpm6YZ0e/nycflights/airports.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.02401853 secs) #> Importing /tmp/Rtmpm6YZ0e/nycflights/flights.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 7.150884 secs) #> Importing /tmp/Rtmpm6YZ0e/nycflights/planes.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.0348866 secs) #> Importing /tmp/Rtmpm6YZ0e/nycflights/weather.tsv.bz2 in 50000 line chunks: #>  ...Done! (in 0.2378168 secs)"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"using-filters","dir":"","previous_headings":"","what":"Using filters","title":"Archive and Unarchive Databases Using Flat Files","text":"package can also used generate slices data required analytical operational purposes. example archive disk flight data occurred month December. recommended use filters single table time.","code":"ark(db, dir, lines = 50000, tables = \"flights\", filter_statement = \"WHERE month = 12\")"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"using-callbacks","dir":"","previous_headings":"","what":"Using callbacks","title":"Archive and Unarchive Databases Using Flat Files","text":"possible use callback perform just--time data transformations ark writes data object disk preferred format. example , write simple transformation convert flights data arr_delay field, minutes, hours. recommended use callbacks single table time. callback function can anything can imagine long returns data.frame can written disk.","code":"mins_to_hours <- function(data) {   data$arr_delay <- data$arr_delay/60   data }  ark(db, dir, lines = 50000, tables = \"flights\", callback = mins_to_hours)"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"ark-in-parallel","dir":"","previous_headings":"","what":"ark() in parallel","title":"Archive and Unarchive Databases Using Flat Files","text":"two strategies using ark parallel. One loop tables, re-using ark function per table parallel. , introduced 0.0.15, use “window-parallel” method loops chunks table. particularly useful tables large can speed process significantly. Note: window-parallel currently works conjunction streamable_parquet","code":"# Strategy 1: Parallel over tables library(arkdb) library(future.apply)  plan(multisession)  # Any streamable_table method is acceptable future_lapply(vector_of_tables, function(x) ark(db, dir, lines, tables = x))  # Strategy 2: Parallel over chunks of a table library(arkdb) library(future.apply)  plan(multisession)  ark(   db,    dir,    streamable_table = streamable_parquet(), # required for window-parallel   lines = 50000,    tables = \"flights\",    method = \"window-parallel\" )  # Strategy 3: Parallel over tables and chunks of tables library(arkdb) library(future.apply) # 16 core machine for example plan(list(tweak(multisession, n = 4), tweak(multisession, n = 4)))  # 4 tables at a time, 4 threads per table future_lapply(vector_of_tables, function(x) {    ark(     db,      dir,      streamable_table = streamable_parquet(), # required for window-parallel     lines = 50000,      tables = x,      method = \"window-parallel\")   } )"},{"path":"https://docs.ropensci.org/arkdb/index.html","id":"etls-with-arkdb","dir":"","previous_headings":"","what":"ETLs with arkdb","title":"Archive and Unarchive Databases Using Flat Files","text":"arkdb package can also used create number ETL pipelines involving text archives databases given ability filter, use callbacks. example , leverage duckdb read fictional folder files US state, filter var_filtered, apply callback transformation transform_fun var_transformed save parquet, load folder parquet files analysis Apache Arrow. Please note project released Contributor Code Conduct. participating project agree abide terms.","code":"library(arrow) library(duckdb)  db <- dbConnect(duckdb::duckdb())  transform_fun <- function(data) {   data$var_transformed <- sqrt(data$var_transformed)   data }  for(state in c(\"DC\", state.abb)) {   path <- paste0(\"path/to/archives/\", state, \".gz\")      ark(     db,     dir = paste0(\"output/\", state),     streamable_table = streamable_parquet(), # parquet files of nline rows     lines = 100000,     # See: https://duckdb.org/docs/data/csv     tables = sprintf(\"read_csv_auto('%s')\", path),      compress = \"none\", # Compression meaningless for parquet as it's already compressed     overwrite = T,      filenames = state, # Overload tablename     filter_statement = \"WHERE var_filtered = 1\",     callback = transform_fun   ) }  # The result is trivial to read in with arrow  ds <- open_dataset(\"output\", partitioning = \"state\")"},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":null,"dir":"Reference","previous_headings":"","what":"Archive tables from a database as flat files — ark","title":"Archive tables from a database as flat files — ark","text":"Archive tables database flat files","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Archive tables from a database as flat files — ark","text":"","code":"ark(   db_con,   dir,   streamable_table = streamable_base_tsv(),   lines = 50000L,   compress = c(\"bzip2\", \"gzip\", \"xz\", \"none\"),   tables = list_tables(db_con),   method = c(\"keep-open\", \"window\", \"window-parallel\", \"sql-window\"),   overwrite = \"ask\",   filter_statement = NULL,   filenames = NULL,   callback = NULL )"},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Archive tables from a database as flat files — ark","text":"db_con database connection dir directory write compressed text files output streamable_table interface serializing/deserializing chunks lines number lines use single chunk compress file compression algorithm. one \"bzip2\" (default), \"gzip\" (faster write times, bit less compression), \"xz\", \"none\", compression. tables list tables database archived.  default, archive tables. Table list specify schema appropriate, see examples. method method use query database, see details. overwrite existing text files name overwritten? default \"ask\", ask confirmation interactive session, overwrite non-interactive script.  TRUE always overwrite, FALSE always skip tables. filter_statement Typically SQL \"\" clause, specific dataset. (e.g., year = 2013) filenames optional vector names used name files instead using tablename tables parameter. callback optional function acts data.frame written disk streamable_table. recommended use single table time. Callback functions must return data.frame.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Archive tables from a database as flat files — ark","text":"path dir output files created (invisibly), piping.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Archive tables from a database as flat files — ark","text":"ark archive tables database (compressed) tsv files. formats streamtable_table method, like parquet. ark reading chunks time memory, allowing process tables large read memory (probably using database first place!)  Compressed text files likely take much less space, making easier store transfer networks.  Compressed plain-text files also archival friendly, rely widely available long-established open source compression algorithms plain text, making less vulnerable loss changes database technology formats. almost cases, default method best choice. DBI::dbSendQuery() implementation database platform returns full results client immediately rather supporting chunking n parameter, may want use \"window\" method, generic.  \"sql-window\" method provides faster alternative databases like PostgreSQL support windowing natively (.e. queries). Note \"window-parallel\" works streamable_parquet.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/ark.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Archive tables from a database as flat files — ark","text":"","code":"# \\donttest{ # setup library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union dir <- tempdir() db <- dbplyr::nycflights13_sqlite(tempdir()) #> Caching nycflights db at /tmp/RtmpkL3rOO/nycflights13.sqlite #> Creating table: airlines #> Creating table: airports #> Creating table: flights #> Creating table: planes #> Creating table: weather  ## And here we go: ark(db, dir) #> Exporting airlines in 50000 line chunks: #> \t...Done! (in 0.003395557 secs) #> Exporting airports in 50000 line chunks: #> \t...Done! (in 0.01148033 secs) #> Exporting flights in 50000 line chunks: #> \t...Done! (in 5.817739 secs) #> Exporting planes in 50000 line chunks: #> \t...Done! (in 0.01576948 secs) #> Exporting weather in 50000 line chunks: #> \t...Done! (in 0.4104521 secs) # } if (FALSE) {  ## For a Postgres DB with schema, we can append schema names first ## to each of the table names, like so: schema_tables <- dbGetQuery(db, sqlInterpolate(db,   \"SELECT table_name FROM information_schema.tables WHERE table_schema = ?schema\",   schema = \"schema_name\" ))  ark(db, dir, tables = paste0(\"schema_name\", \".\", schema_tables$table_name)) }"},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb-package.html","id":null,"dir":"Reference","previous_headings":"","what":"arkdb: Archive and Unarchive Databases Using Flat Files — arkdb-package","title":"arkdb: Archive and Unarchive Databases Using Flat Files — arkdb-package","text":"Flat text files provide robust, compressible, portable way store tables.  package provides convenient functions exporting tables relational database connections compressed text files streaming text files back database without requiring whole table fit working memory.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"arkdb: Archive and Unarchive Databases Using Flat Files — arkdb-package","text":"two functions: ark(): archive database flat files, chunk chunk. unark(): Unarchive flat files back int database connection. arkdb work DBI supported connection.  makes convenient robust way migrate different databases well.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"arkdb: Archive and Unarchive Databases Using Flat Files — arkdb-package","text":"Maintainer: Carl Boettiger cboettig@gmail.com (ORCID) [copyright holder] contributors: Richard FitzJohn [contributor] Brandon Bertelsen brandon@bertelsen.ca [contributor]","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb_delete_db.html","id":null,"dir":"Reference","previous_headings":"","what":"delete the local arkdb database — arkdb_delete_db","title":"delete the local arkdb database — arkdb_delete_db","text":"delete local arkdb database","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb_delete_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"delete the local arkdb database — arkdb_delete_db","text":"","code":"arkdb_delete_db(db_dir = arkdb_dir(), ask = interactive())"},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb_delete_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"delete the local arkdb database — arkdb_delete_db","text":"db_dir neon database location ask Ask confirmation first?","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb_delete_db.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"delete the local arkdb database — arkdb_delete_db","text":"Just helper function deletes database files.  Usually unnecessary can helpful resetting corrupt database.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/arkdb_delete_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"delete the local arkdb database — arkdb_delete_db","text":"","code":"# Create a db dir <- tempfile() db <- local_db(dir)  # Delete it arkdb_delete_db(dir, ask = FALSE)"},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to a local stand-alone database — local_db","title":"Connect to a local stand-alone database — local_db","text":"function provide connection best available database. function drop-replacement [DBI::dbConnect] behaviour makes subtle R packages need database backend minimal complexity, described details.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to a local stand-alone database — local_db","text":"","code":"local_db(   dbdir = arkdb_dir(),   driver = Sys.getenv(\"ARKDB_DRIVER\", \"duckdb\"),   readonly = FALSE,   cache_connection = TRUE,   memory_limit = getOption(\"duckdb_memory_limit\", NA),   ... )"},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to a local stand-alone database — local_db","text":"dbdir Path database. driver Default driver, one \"duckdb\", \"MonetDBLite\", \"RSQLite\". select first one finds available driver set. fallback can overwritten either explicit argument setting environmental variable ARKDB_DRIVER. readonly database opened read-? (duckdb ). allows multiple concurrent connections (e.g. different R sessions) cache_connection preserve cache connection? allows faster load times prevents connection garbage-collected.  However, keeping open read-write connection duckdb MonetDBLite block access R sessions database. memory_limit Set memory limit duckdb, GB.  can also set session using options, e.g. options(duckdb_memory_limit=10) limit 10GB.  systems duckdb automatically set limit 80% machine capacity set explicitly. ... additional arguments (used time)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to a local stand-alone database — local_db","text":"Returns [DBIconnection] connection default database","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Connect to a local stand-alone database — local_db","text":"function provides several abstractions [DBI::dbConnect] provide seamless backend use inside R packages. First,  provides generic method allows use [RSQLite::SQLite]`` connection nothing else available, able automatically select much faster, powerful backend duckdb::duckdb` available.  argument environmental variable can used override manually set database endpoint testing purposes. Second, function cache database connection R environment load cache.  means can call local_db() fast/frequently like without causing errors occur rapid calls [DBI::dbConnect] Third, function defaults persistent storage location set [tools::R_user_dir] configurable setting environmental variable ARKDB_HOME.  allows package provide persistent storage ---box, easily switch storage temporary directory (e.g. testing purposes, custom user configuration) without edit database calls directly.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to a local stand-alone database — local_db","text":"","code":"# \\donttest{ ## OPTIONAL: you can first set an alternative home location, ## such as a temporary directory: Sys.setenv(ARKDB_HOME = tempdir())  ## Connect to the database: db <- local_db() # }"},{"path":"https://docs.ropensci.org/arkdb/reference/local_db_disconnect.html","id":null,"dir":"Reference","previous_headings":"","what":"Disconnect from the arkdb database. — local_db_disconnect","title":"Disconnect from the arkdb database. — local_db_disconnect","text":"Disconnect arkdb database.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db_disconnect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disconnect from the arkdb database. — local_db_disconnect","text":"","code":"local_db_disconnect(db = local_db(), env = arkdb_cache)"},{"path":"https://docs.ropensci.org/arkdb/reference/local_db_disconnect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Disconnect from the arkdb database. — local_db_disconnect","text":"db DBI connection. default, call local_db default connection. env environment function looks connection.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db_disconnect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Disconnect from the arkdb database. — local_db_disconnect","text":"function manually closes connection arkdb database.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/local_db_disconnect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Disconnect from the arkdb database. — local_db_disconnect","text":"","code":"# \\donttest{  ## Disconnect from the database: local_db_disconnect() # }"},{"path":"https://docs.ropensci.org/arkdb/reference/process_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"process a table in chunks — process_chunks","title":"process a table in chunks — process_chunks","text":"process table chunks","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/process_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"process a table in chunks — process_chunks","text":"","code":"process_chunks(   file,   process_fn,   streamable_table = NULL,   lines = 50000L,   encoding = Sys.getenv(\"encoding\", \"UTF-8\"),   ... )"},{"path":"https://docs.ropensci.org/arkdb/reference/process_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"process a table in chunks — process_chunks","text":"file path file process_fn function chunk streamable_table interface serializing/deserializing chunks lines number lines read chunk. encoding encoding assumed input files. ... additional arguments streamable_table$read method.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/process_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"process a table in chunks — process_chunks","text":"","code":"con <- system.file(\"extdata/mtcars.tsv.gz\", package = \"arkdb\") dummy <- function(x) message(paste(dim(x), collapse = \" x \")) process_chunks(con, dummy, lines = 8) #> Importing /usr/local/lib/R/site-library/arkdb/extdata/mtcars.tsv.gz in 8 line chunks: #> 8 x 11 #> 8 x 11 #> 8 x 11 #> 8 x 11 #> \t...Done! (in 0.002804041 secs)"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable csv using base R functions — streamable_base_csv","title":"streamable csv using base R functions — streamable_base_csv","text":"streamable csv using base R functions","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable csv using base R functions — streamable_base_csv","text":"","code":"streamable_base_csv()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable csv using base R functions — streamable_base_csv","text":"streamable_table object (S3)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_csv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"streamable csv using base R functions — streamable_base_csv","text":"Follows comma-separate-values standard using utils::read.table()","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_tsv.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable tsv using base R functions — streamable_base_tsv","title":"streamable tsv using base R functions — streamable_base_tsv","text":"streamable tsv using base R functions","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_tsv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable tsv using base R functions — streamable_base_tsv","text":"","code":"streamable_base_tsv()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_tsv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable tsv using base R functions — streamable_base_tsv","text":"streamable_table object (S3)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_base_tsv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"streamable tsv using base R functions — streamable_base_tsv","text":"Follows tab-separate-values standard using utils::read.table(), see IANA specification : https://www.iana.org/assignments/media-types/text/tab-separated-values","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_parquet.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable chunked parquet using arrow — streamable_parquet","title":"streamable chunked parquet using arrow — streamable_parquet","text":"streamable chunked parquet using arrow","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_parquet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable chunked parquet using arrow — streamable_parquet","text":"","code":"streamable_parquet()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_parquet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable chunked parquet using arrow — streamable_parquet","text":"streamable_table object (S3)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_parquet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"streamable chunked parquet using arrow — streamable_parquet","text":"Parquet files streamed disk breaking chunks equal nlines parameter initial call ark. tablename, folder created chunks placed folder form part-000000.parquet. software looks folder, increments name appropriately next chunk. done intentionally users can take advantage arrow::open_dataset future, coming back review perform analysis data.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable csv using readr — streamable_readr_csv","title":"streamable csv using readr — streamable_readr_csv","text":"streamable csv using readr","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable csv using readr — streamable_readr_csv","text":"","code":"streamable_readr_csv()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable csv using readr — streamable_readr_csv","text":"streamable_table object (S3)","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_tsv.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable tsv using readr — streamable_readr_tsv","title":"streamable tsv using readr — streamable_readr_tsv","text":"streamable tsv using readr","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_tsv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable tsv using readr — streamable_readr_tsv","text":"","code":"streamable_readr_tsv()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_readr_tsv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable tsv using readr — streamable_readr_tsv","text":"streamable_table object (S3)","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable table — streamable_table","title":"streamable table — streamable_table","text":"streamable table","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable table — streamable_table","text":"","code":"streamable_table(read, write, extension)"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"streamable table — streamable_table","text":"read read function. Arguments \"file\" (must able take connection() object) \"...\" () additional arguments. write write function. Arguments \"data\" (data.frame), file (must able take connection() object), \"omit_header\" logical, include header (initial write) (appending subsequent chunks) extension file extension use (e.g. \"tsv\", \"csv\")","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable table — streamable_table","text":"streamable_table object (S3)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"streamable table — streamable_table","text":"Note several constraints design. write method must able take generic R connection object (allow handle compression methods used, ), read method must able take textConnection object.  readr functions handle cases box, method easy write.  Also note write method must able omit_header. See built-methods examples.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"streamable table — streamable_table","text":"","code":"streamable_readr_tsv <- function() {   streamable_table(     function(file, ...) readr::read_tsv(file, ...),     function(x, path, omit_header) {       readr::write_tsv(x = x, path = path, omit_header = omit_header)     },     \"tsv\"   ) }"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_vroom.html","id":null,"dir":"Reference","previous_headings":"","what":"streamable tables using vroom — streamable_vroom","title":"streamable tables using vroom — streamable_vroom","text":"streamable tables using vroom","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_vroom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"streamable tables using vroom — streamable_vroom","text":"","code":"streamable_vroom()"},{"path":"https://docs.ropensci.org/arkdb/reference/streamable_vroom.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"streamable tables using vroom — streamable_vroom","text":"streamable_table object (S3)","code":""},{"path":[]},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":null,"dir":"Reference","previous_headings":"","what":"Unarchive a list of compressed tsv files into a database — unark","title":"Unarchive a list of compressed tsv files into a database — unark","text":"Unarchive list compressed tsv files database","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unarchive a list of compressed tsv files into a database — unark","text":"","code":"unark(   files,   db_con,   streamable_table = NULL,   lines = 50000L,   overwrite = \"ask\",   encoding = Sys.getenv(\"encoding\", \"UTF-8\"),   tablenames = NULL,   try_native = TRUE,   ... )"},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unarchive a list of compressed tsv files into a database — unark","text":"files vector filenames read . Must tsv format, optionally compressed using bzip2, gzip, zip, xz format present. db_con database src (src_dbi object dplyr) streamable_table interface serializing/deserializing chunks lines number lines read chunk. overwrite existing text files name overwritten? default \"ask\", ask confirmation interactive session, overwrite non-interactive script.  TRUE always overwrite, FALSE always skip tables. encoding encoding assumed input files. tablenames vector tablenames used corresponding files. default, tables named using lowercase names file basename special characters replaced underscores (SQL compatibility). try_native logical, default TRUE. try use native bulk import method database connection?  can substantially speed read times fall back DBI method table fails import.  Currently MonetDBLite connections support . ... additional arguments streamable_table$read method.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unarchive a list of compressed tsv files into a database — unark","text":"database connection (invisibly)","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Unarchive a list of compressed tsv files into a database — unark","text":"unark read files chunks write database.  essential processing large compressed tables may large read memory writing database.  general, increasing lines parameter result faster total transfer require free memory working larger chunks. using readr-based streamable-table, can suppress progress bar using options(readr.show_progress = FALSE) reading large files.","code":""},{"path":"https://docs.ropensci.org/arkdb/reference/unark.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unarchive a list of compressed tsv files into a database — unark","text":"","code":"# \\donttest{ ## Setup: create an archive. library(dplyr) dir <- tempdir() db <- dbplyr::nycflights13_sqlite(tempdir())  ## database -> .tsv.bz2 ark(db, dir) #> Warning: overwriting airlines.tsv.bz2 #> Exporting airlines in 50000 line chunks: #> \t...Done! (in 0.001766443 secs) #> Warning: overwriting airports.tsv.bz2 #> Exporting airports in 50000 line chunks: #> \t...Done! (in 0.01085544 secs) #> Warning: overwriting flights.tsv.bz2 #> Exporting flights in 50000 line chunks: #> \t...Done! (in 5.796999 secs) #> Warning: overwriting planes.tsv.bz2 #> Exporting planes in 50000 line chunks: #> \t...Done! (in 0.01606011 secs) #> Warning: overwriting weather.tsv.bz2 #> Exporting weather in 50000 line chunks: #> \t...Done! (in 0.4103391 secs)  ## list all files in archive (full paths) files <- list.files(dir, \"bz2$\", full.names = TRUE)  ## Read archived files into a new database (another sqlite in this case) new_db <- DBI::dbConnect(RSQLite::SQLite()) unark(files, new_db) #> Importing /tmp/RtmpkL3rOO/airlines.tsv.bz2 in 50000 line chunks: #> \t...Done! (in 0.00699544 secs) #> Importing /tmp/RtmpkL3rOO/airports.tsv.bz2 in 50000 line chunks: #> \t...Done! (in 0.01344156 secs) #> Importing /tmp/RtmpkL3rOO/flights.tsv.bz2 in 50000 line chunks: #> \t...Done! (in 3.539262 secs) #> Importing /tmp/RtmpkL3rOO/planes.tsv.bz2 in 50000 line chunks: #> \t...Done! (in 0.01919818 secs) #> Importing /tmp/RtmpkL3rOO/weather.tsv.bz2 in 50000 line chunks: #> \t...Done! (in 0.1175041 secs)  ## Prove table is returned successfully. tbl(new_db, \"flights\") #> # Source:   table<flights> [?? x 19] #> # Database: sqlite 3.45.0 [] #>     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time #>    <int> <int> <int>    <int>          <int>     <int>    <int>          <int> #>  1  2013     1     1      517            515         2      830            819 #>  2  2013     1     1      533            529         4      850            830 #>  3  2013     1     1      542            540         2      923            850 #>  4  2013     1     1      544            545        -1     1004           1022 #>  5  2013     1     1      554            600        -6      812            837 #>  6  2013     1     1      554            558        -4      740            728 #>  7  2013     1     1      555            600        -5      913            854 #>  8  2013     1     1      557            600        -3      709            723 #>  9  2013     1     1      557            600        -3      838            846 #> 10  2013     1     1      558            600        -2      753            745 #> # ℹ more rows #> # ℹ 11 more variables: arr_delay <int>, carrier <chr>, flight <int>, #> #   tailnum <chr>, origin <chr>, dest <chr>, air_time <int>, distance <int>, #> #   hour <int>, minute <int>, time_hour <dbl> # }"},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0018","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.18","title":"arkdb 0.0.18","text":"CRAN release: 2024-01-15 patch test infrastructure handling soft dependency arrow","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0017","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.17","title":"arkdb 0.0.17","text":"CRAN release: 2024-01-08 patch test infrastructure Windows","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0016","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.16","title":"arkdb 0.0.16","text":"CRAN release: 2022-09-09 patch local_db() defaulting path subdir. update roxygen","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0015","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.15","title":"arkdb 0.0.15","text":"CRAN release: 2022-02-15 Added window-parallel option ark’ing large tables parallel conditional testing M1/arm Mac","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0014","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.14","title":"arkdb 0.0.14","text":"CRAN release: 2021-10-18 Patch test suite Solaris. arrow package installs Solaris, functions actually run correctly since C++ libraries set properly Solaris.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0013","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.13","title":"arkdb 0.0.13","text":"CRAN release: 2021-10-15 Added ability name output files directly. Add warning users specify compression parquet files. Added callback functionality ark function. Allowing users perform transformations recodes chunked data.frames saved disk. Added ability filter databases allowing users specify “” clause. Added parquet streamable_table format, allowing users ark parquet instead text format.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0012","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.12","title":"arkdb 0.0.12","text":"CRAN release: 2021-04-05 Bugfix arkdb","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0011","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.11","title":"arkdb 0.0.11","text":"CRAN release: 2021-03-13 Make cached connection opt-instead applying read_only. allows cache work read-write connections default. also avoids condition connection garbage-collected functions call local_db internally.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0010","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.10","title":"arkdb 0.0.10","text":"CRAN release: 2021-03-05 Better handling read_only vs read_write connections. caches read_only connections. Includes optional support MonetDBLite","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-008","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.8","title":"arkdb 0.0.8","text":"CRAN release: 2020-11-04 Bugfix dplyr 2.0.0 release","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-007","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.7","title":"arkdb 0.0.7","text":"CRAN release: 2020-10-15 Bugfix upcoming dplyr 2.0.0 release","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-006","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.6","title":"arkdb 0.0.6","text":"CRAN release: 2020-09-18 Support vroom opt-streamable table Export process_chunks Add mechanism attempt bulk importer, available (#27) Bugfix case text contains # characters base parser (#28) Lighten core dependencies. Fully recursive dependencies include 4 non-base packages now, progress now optional. Use “magic numbers” instead extensions guess compression type. (NOTE: requires file local URL) Now duckdb CRAN MonetDBLite isn’t, drop built-support MonetDBLite favor duckdb alone.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-005-2018-10-31","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.5 2018-10-31","title":"arkdb 0.0.5 2018-10-31","text":"CRAN release: 2018-10-31 ark()’s default keep-open method cut header names Postgres connections (due variation behavior SQL queries LIMIT 0.) issue now resolved accessing header robust, general way.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-004-2018-09-27","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.4 2018-09-27","title":"arkdb 0.0.4 2018-09-27","text":"CRAN release: 2018-09-27 unark() strip non-compliant characters table names default. unark() gains optional argument tablenames, allowing user specify corresponding table names manually, rather enforcing correspond incoming file names. #18 unark() gains argument encoding, allowing users directly set encoding incoming files. Previously set setting options(encoding), still work well. See FAO.R example examples illustration. unark() now attempt guess streaming parser use (e.g csv tsv) based file extension pattern, rather defaulting tsv parser. (ark() still defaults exporting portable tsv format).","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-003-2018-09-11","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.3 2018-09-11","title":"arkdb 0.0.3 2018-09-11","text":"CRAN release: 2018-09-11 Remove dependency utils::askYesNo backward compatibility, #17","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-002-2018-08-20-first-release-to-cran","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.2 2018-08-20 (First release to CRAN)","title":"arkdb 0.0.2 2018-08-20 (First release to CRAN)","text":"CRAN release: 2018-08-20 Ensure suggested dependency MonetDBLite available running unit test using .","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-001-2018-08-20","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.1 2018-08-20","title":"arkdb 0.0.1 2018-08-20","text":"CRAN release: 2018-08-20 Overwrite existing tables name (warning interactive proceed) DB text-files avoid appending.","code":""},{"path":"https://docs.ropensci.org/arkdb/news/index.html","id":"arkdb-0009000-2018-08-11","dir":"Changelog","previous_headings":"","what":"arkdb 0.0.0.9000 2018-08-11","title":"arkdb 0.0.0.9000 2018-08-11","text":"Added NEWS.md file track changes package. Log messages improved suggested @richfitz Improved mechanism windowing DBs, @krlmlr #8 Support pluggable /O, based @richfitz suggestions #3, #10","code":""}]
